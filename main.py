#!/usr/bin/env python3
"""
AI Newsletter Curation Agent using Strands SDK and Ollama
Curates tech/AI newsletter content and generates markdown output
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests
from bs4 import BeautifulSoup
import feedparser
from urllib.parse import urlparse, urljoin
import re

# Strands SDK imports
from strands import Agent, tool
from strands.models.ollama import OllamaModel
from strands_tools import file_write

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NewsletterConfig:
    """Configuration for the newsletter curation agent"""

    def __init__(self):
        self.sources = [
            "https://feeds.feedburner.com/oreilly/radar",
            "https://news.ycombinator.com/rss",
            "https://techcrunch.com/feed/",
            "https://www.theverge.com/rss/index.xml",
            "https://venturebeat.com/feed/",
            "https://www.wired.com/feed/rss"
        ]
        self.max_articles = 20
        self.lookback_days = 7
        self.ai_keywords = [
            "artificial intelligence", "machine learning", "AI", "ML", "LLM",
            "large language model", "generative AI", "deep learning", "neural network",
            "transformer", "GPT", "Claude", "ChatGPT", "automation", "robotics"
        ]
        self.tech_keywords = [
            "technology", "software", "programming", "startup", "tech company",
            "cloud computing", "cybersecurity", "blockchain", "cryptocurrency",
            "web development", "mobile app", "API", "database", "DevOps"
        ]


@tool
def fetch_rss_feeds(sources: List[str], max_articles: int = 50) -> List[Dict[str, Any]]:
    """
    Fetch articles from RSS feeds

    Args:
        sources: List of RSS feed URLs
        max_articles: Maximum number of articles to fetch per source

    Returns:
        List of article dictionaries with title, link, description, date
    """
    articles = []

    for source in sources:
        try:
            logger.info(f"Fetching RSS feed: {source}")
            feed = feedparser.parse(source)

            for entry in feed.entries[:max_articles]:
                article = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "description": entry.get("summary", ""),
                    "published": entry.get("published", ""),
                    "source": source,
                    "content": ""
                }
                articles.append(article)

        except Exception as e:
            logger.error(f"Error fetching RSS feed {source}: {e}")
            continue

    return articles


@tool
def scrape_article_content(url: str) -> str:
    """
    Scrape the main content from an article URL

    Args:
        url: Article URL to scrape

    Returns:
        Extracted article text content
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Try to find main content areas
        content_selectors = [
            'article', '[role="main"]', '.article-content',
            '.post-content', '.entry-content', '.content'
        ]

        content = ""
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True, separator='\n')
                break

        if not content:
            # Fallback to body content
            content = soup.get_text(strip=True, separator='\n')

        # Clean up content
        content = re.sub(r'\n\s*\n', '\n\n', content)
        return content[:3000]  # Limit content length

    except Exception as e:
        logger.error(f"Error scraping article {url}: {e}")
        return ""


@tool
def filter_relevant_articles(articles: List[Dict], keywords: List[str], max_results: int = 10) -> List[Dict]:
    """
    Filter articles based on relevance to AI/tech keywords

    Args:
        articles: List of article dictionaries
        keywords: Keywords to filter by
        max_results: Maximum number of articles to return

    Returns:
        Filtered and scored list of articles
    """
    scored_articles = []

    for article in articles:
        score = 0
        text = f"{article.get('title', '')} {article.get('description', '')}".lower()

        # Score based on keyword matches
        for keyword in keywords:
            if keyword.lower() in text:
                score += 1

        if score > 0:
            article['relevance_score'] = score
            scored_articles.append(article)

    # Sort by relevance score
    scored_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
    return scored_articles[:max_results]


@tool
def generate_newsletter_markdown(articles: List[Dict], newsletter_title: str = "AI & Tech Weekly") -> str:
    """
    Generate a markdown newsletter from curated articles

    Args:
        articles: List of curated article dictionaries
        newsletter_title: Title for the newsletter

    Returns:
        Formatted markdown newsletter content
    """
    date_str = datetime.now().strftime("%B %d, %Y")

    markdown = f"""# {newsletter_title}
*{date_str}*

Welcome to this week's curated selection of AI and technology news. Here are the most important stories and developments:

## üî• Top Stories
"""

    for i, article in enumerate(articles[:5], 1):
        markdown += f"""### {i}. {article['title']}

**Source:** {urlparse(article['link']).netloc}  
**Link:** [{article['title']}]({article['link']})

{article.get('description', 'No description available.')}

---

"""

    if len(articles) > 5:
        markdown += "\n## üìñ Additional Reading\n\n"
        for article in articles[5:]:
            markdown += f"- [{article['title']}]({article['link']}) - *{urlparse(article['link']).netloc}*\n"

    markdown += f"""

## ü§ñ About This Newsletter

This newsletter was curated by an AI agent using the latest developments in artificial intelligence and technology. 
Articles were automatically selected based on relevance, novelty, and potential impact.

*Generated on {date_str}*
"""

    return markdown


class NewsletterCurationAgent:
    """Main agent class for newsletter curation"""

    def __init__(self, ollama_host: str = "http://localhost:11434", model_id: str = "qwen2.5:3b"):
        self.config = NewsletterConfig()

        # Initialize Ollama model
        self.ollama_model = OllamaModel(
            host=ollama_host,
            model_id=model_id,
            temperature=0.3,  # Lower temperature for more consistent output
            keep_alive="10m"
        )

        # Create agent with tools
        self.agent = Agent(
            model=self.ollama_model,
            tools=[
                fetch_rss_feeds,
                scrape_article_content,
                filter_relevant_articles,
                generate_newsletter_markdown,
                file_write
            ]
        )

    def _generate_newsletter_markdown_direct(self, articles: List[Dict],
                                             newsletter_title: str = "AI & Tech Weekly") -> str:
        """Generate newsletter markdown directly (not as a tool)"""
        date_str = datetime.now().strftime("%B %d, %Y")

        markdown = f"""# {newsletter_title}
    *{date_str}*

    Welcome to this week's curated selection of AI and technology news. Here are the most important stories and developments:

    ## üî• Top Stories

    """

        for i, article in enumerate(articles[:5], 1):
            source_domain = urlparse(article['link']).netloc
            markdown += f"""### {i}. {article['title']}

    **Source:** {source_domain}  
    **Link:** [{article['title']}]({article['link']})

    {article.get('description', 'No description available.')}

    ---

    """

        if len(articles) > 5:
            markdown += "\n## üìñ Additional Reading\n\n"
            for article in articles[5:]:
                source_domain = urlparse(article['link']).netloc
                markdown += f"- [{article['title']}]({article['link']}) - *{source_domain}*\n"

        markdown += f"""

    ## ü§ñ About This Newsletter

    This newsletter was curated by an AI agent using the latest developments in artificial intelligence and technology. 
    Articles were automatically selected based on relevance, novelty, and potential impact.

    *Generated on {date_str}*
    """

        return markdown

    def _save_newsletter_direct(self, content: str, filename: str) -> str:
        """Save newsletter directly (not as a tool)"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"Newsletter saved to {filename}")
            return f"Newsletter successfully saved to {filename}"
        except Exception as e:
            error_msg = f"Error saving newsletter: {e}"
            logger.error(error_msg)
            return error_msg

    def _fetch_rss_feeds_direct(self, sources: List[str], max_articles: int = 50) -> List[Dict[str, Any]]:
        """Direct RSS fetching (copy the logic from fetch_rss_feeds)"""
        articles = []
        for source in sources:
            try:
                logger.info(f"Fetching RSS feed: {source}")
                feed = feedparser.parse(source)
                for entry in feed.entries[:max_articles]:
                    article = {
                        "title": entry.get("title", ""),
                        "link": entry.get("link", ""),
                        "description": entry.get("summary", ""),
                        "published": entry.get("published", ""),
                        "source": source,
                        "content": ""
                    }
                    articles.append(article)
            except Exception as e:
                logger.error(f"Error fetching RSS feed {source}: {e}")
                continue
        return articles

    def curate_newsletter(self, output_filename: str = "newsletter.md") -> str:
        """
        Main method to curate and generate newsletter

        Args:
            output_filename: Filename for the generated newsletter

        Returns:
            Path to the generated newsletter file
        """
        logger.info("Starting newsletter curation process...")

        try:
            # Step 1: Fetch articles from RSS feeds
            logger.info("Fetching articles from RSS feeds...")
            articles = fetch_rss_feeds(self.config.sources, self.config.max_articles)
            logger.info(f"Fetched {len(articles)} articles")

            # Step 2: Filter for AI/tech relevance
            logger.info("Filtering articles for relevance...")
            all_keywords = self.config.ai_keywords + self.config.tech_keywords
            relevant_articles = filter_relevant_articles(articles, all_keywords, 15)
            logger.info(f"Found {len(relevant_articles)} relevant articles")

            # Step 3: Enhance top articles with full content
            logger.info("Enhancing top articles with full content...")
            for article in relevant_articles[:5]:
                content = scrape_article_content(article['link'])
                article['content'] = content

            # Step 4: Use AI agent to analyze and rank articles
            analysis_prompt = f"""
            Analyze these {len(relevant_articles)} articles and help me create a newsletter. 
            Consider factors like:
            - Novelty and impact of the story
            - Relevance to AI and technology trends
            - Quality of the source
            - Potential interest to tech professionals

            Articles data: {json.dumps(relevant_articles[:10], indent=2)}

            Please suggest the top 5 articles for the newsletter and provide brief explanations for why each is significant.
            """

            logger.info("Getting AI analysis of articles...")
            analysis = self.agent(analysis_prompt)
            logger.info("AI analysis completed")

            # Step 5: Generate newsletter markdown
            logger.info("Generating newsletter markdown...")
            print(
                f"DEBUG: type of self._generate_newsletter_markdown_direct: {type(self._generate_newsletter_markdown_direct)}")
            print(
                f"DEBUG: relevant_articles type: {type(relevant_articles)}, length: {len(relevant_articles) if relevant_articles else 'None'}")

            try:
                newsletter_content = self._generate_newsletter_markdown_direct(relevant_articles)
                print(f"DEBUG: newsletter_content generated successfully, type: {type(newsletter_content)}")
            except Exception as e:
                print(f"DEBUG: Error in _generate_newsletter_markdown_direct: {e}")
                import traceback
                traceback.print_exc()

            # from newsletter_tools import generate_newsletter_markdown as gen_newsletter
            # newsletter_content = self._generate_newsletter_markdown_direct(relevant_articles)

            # Step 6: Save newsletter to file
            save_result = self._save_newsletter_direct(newsletter_content, output_filename)
            logger.info(save_result)

            # file_write(output_filename, newsletter_content)
            logger.info(f"Newsletter saved to {output_filename}")

            return output_filename

        except Exception as e:
            logger.error(f"Error in newsletter curation: {e}")
            raise


def main():
    """Main execution function"""
    try:
        # Initialize the newsletter agent
        agent = NewsletterCurationAgent()

        # Generate newsletter
        print("ü§ñ Starting AI Newsletter Curation Agent...")
        print("üì° Fetching latest AI and tech news...")

        output_file = agent.curate_newsletter("ai_tech_newsletter.md")

        print(f"‚úÖ Newsletter generated successfully: {output_file}")
        print("üìÑ Check the markdown file for your curated newsletter!")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        logger.error(f"Main execution error: {e}")


if __name__ == "__main__":
    main()